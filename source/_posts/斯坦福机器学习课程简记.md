---
layout: pages
title: 斯坦福机器学习课程简记
date: 2018-06-09 16:46:09
tags: Maching Learning
categories: 猿笔记
description: 借大佬春风，正式踏上机器学习之路
---
<pre>
  很久没有更新过博客了，之前把零零碎碎的知识点都记在了云笔记，现在还是开启面向博客学习吧，有助于和大佬们的交流。虽然回过头看之前的博客笔记感觉都是垃圾，但这记录了我的成长之路。
  此处主要记录视频中讲到的不太熟悉的知识点关键字
</pre>
***
## 不明觉厉
-  梯度下降（Gradient Descent）,随机梯度下降（Stochastic），批梯度下降算法(batch gradient descent)
- 叙述性属性转化为k维向量

## 遇到问题
- 线性回归特征变量X次数都为1？
- 如果次数为1（有参数学习），每次迭代又只是修改特征参数W的值，降低损失，按理说可以直接求出损
失函数关于W的导数，使其为0  即可，为什么要用梯
度下降？并且在周志华西瓜书P56中写除了线性回归的最终模型，这是否
表示可以不使用梯度下降而直接套用公式？而且如果是
线性，怎么会出现过拟合的情况呢？我理解的过拟合应该是X次数过高导致模型过于复杂。
- 如果次数有不为1的项那就更奇观了，w的维度将不确定（同一个特征的不同次数项对应不同的特征参数w）
- 不知道哪儿理解出了问题，后面再来一一订正